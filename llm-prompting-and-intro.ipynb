{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajatgarg01/LLM/blob/main/llm-prompting-and-intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "GIOEGX8t2TIW"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command will install the google-generativeai package, ensuring the version is 0.8.3 or higher, and will suppress unnecessary output due to the -q flag. If you encounter any issues during installation, feel free to ask!"
      ],
      "metadata": {
        "id": "IQPqNr0P2TIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T15:43:30.238660Z",
          "iopub.execute_input": "2024-11-14T15:43:30.239142Z",
          "iopub.status.idle": "2024-11-14T15:43:59.805970Z",
          "shell.execute_reply.started": "2024-11-14T15:43:30.239092Z",
          "shell.execute_reply": "2024-11-14T15:43:59.804598Z"
        },
        "id": "7v2lAJoT2TIZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import Markdown,display"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T15:47:11.374500Z",
          "iopub.execute_input": "2024-11-14T15:47:11.374910Z",
          "iopub.status.idle": "2024-11-14T15:47:11.380391Z",
          "shell.execute_reply.started": "2024-11-14T15:47:11.374870Z",
          "shell.execute_reply": "2024-11-14T15:47:11.379080Z"
        },
        "id": "y4j7E8WL2TIa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "HTML: The heading \"Welcome to IPython Display Examples!\" will be displayed in green color using HTML formatting.\n",
        "Markdown: A bulleted list, link, code snippet, and blockquote are demonstrated using Markdown syntax.\n",
        "display: This function is used to render both html_content and markdown_content in a single output cell.\n",
        "This code should work seamlessly in a Jupyter Notebook, giving you a combined output of HTML and Markdown elements!"
      ],
      "metadata": {
        "id": "l_RY-ldZ2TIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "GOOGLE_API_KEY=UserSecretsClient().get_secret(\"google_api_key\")\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T15:52:09.077462Z",
          "iopub.execute_input": "2024-11-14T15:52:09.077953Z",
          "iopub.status.idle": "2024-11-14T15:52:09.244555Z",
          "shell.execute_reply.started": "2024-11-14T15:52:09.077896Z",
          "shell.execute_reply": "2024-11-14T15:52:09.243359Z"
        },
        "id": "XQt7OlEM2TIb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet you've posted suggests that you are trying to access a Google API key securely using the UserSecretsClient and configure the genai library with that key. Here's a breakdown of how the code works:\n",
        "\n",
        "Code Explanation\n",
        "UserSecretsClient: This is often used in environments like Jupyter Notebooks (e.g., Azure Notebooks or JupyterHub) where secrets (like API keys) are stored securely. You retrieve a secret using the get_secret method, which fetches a secret (in this case, \"GOOGLE_API_KEY\").\n",
        "\n",
        "API Configuration: The genai.configure() function is used to set up the google-generativeai library with the retrieved API key for making authenticated API requests"
      ],
      "metadata": {
        "id": "KrZNQ0as2TIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIRST PROMPT**"
      ],
      "metadata": {
        "id": "IJ_D0QIp2TIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flash=genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "response=flash.generate_content(\"Explain AI to me like I'm a kid\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T15:55:37.649734Z",
          "iopub.execute_input": "2024-11-14T15:55:37.650128Z",
          "iopub.status.idle": "2024-11-14T15:55:39.148431Z",
          "shell.execute_reply.started": "2024-11-14T15:55:37.650090Z",
          "shell.execute_reply": "2024-11-14T15:55:39.147370Z"
        },
        "id": "BE9QBRLF2TIc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T15:56:20.669672Z",
          "iopub.execute_input": "2024-11-14T15:56:20.670130Z",
          "iopub.status.idle": "2024-11-14T15:56:20.679703Z",
          "shell.execute_reply.started": "2024-11-14T15:56:20.670088Z",
          "shell.execute_reply": "2024-11-14T15:56:20.678354Z"
        },
        "id": "q8tFXZ0G2TIc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***START A CHAT***"
      ],
      "metadata": {
        "id": "eX2EjMAy2TIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat=flash.start_chat(history=[])\n",
        "response=chat.send_message('Hello! my name is rajat garg')\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T16:00:18.988919Z",
          "iopub.execute_input": "2024-11-14T16:00:18.989452Z",
          "iopub.status.idle": "2024-11-14T16:00:19.452810Z",
          "shell.execute_reply.started": "2024-11-14T16:00:18.989394Z",
          "shell.execute_reply": "2024-11-14T16:00:19.451659Z"
        },
        "id": "GsZaz6Tc2TId"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "response=chat.send_message('can you tell me something intresting about you')\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T16:01:55.412368Z",
          "iopub.execute_input": "2024-11-14T16:01:55.412995Z",
          "iopub.status.idle": "2024-11-14T16:01:56.529642Z",
          "shell.execute_reply.started": "2024-11-14T16:01:55.412938Z",
          "shell.execute_reply": "2024-11-14T16:01:56.528296Z"
        },
        "id": "KHWZXboA2TId"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T16:02:15.513836Z",
          "iopub.execute_input": "2024-11-14T16:02:15.514422Z",
          "iopub.status.idle": "2024-11-14T16:02:15.523467Z",
          "shell.execute_reply.started": "2024-11-14T16:02:15.514364Z",
          "shell.execute_reply": "2024-11-14T16:02:15.522152Z"
        },
        "id": "G8RsyhQu2TId"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "response=chat.send_message('do you still remember my name')\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T16:03:28.689836Z",
          "iopub.execute_input": "2024-11-14T16:03:28.690399Z",
          "iopub.status.idle": "2024-11-14T16:03:29.317308Z",
          "shell.execute_reply.started": "2024-11-14T16:03:28.690339Z",
          "shell.execute_reply": "2024-11-14T16:03:29.315899Z"
        },
        "id": "V9CIwExk2TId"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHOOSE A MODEL**"
      ],
      "metadata": {
        "id": "kIxJtKw02TId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "    print(model.name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T16:05:28.920741Z",
          "iopub.execute_input": "2024-11-14T16:05:28.921204Z",
          "iopub.status.idle": "2024-11-14T16:05:29.084888Z",
          "shell.execute_reply.started": "2024-11-14T16:05:28.921158Z",
          "shell.execute_reply": "2024-11-14T16:05:29.083491Z"
        },
        "id": "qu44o3Ax2TIe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a description of each AI model you mentioned, categorized based on their functions:\n",
        "\n",
        "*Chat Models*\n",
        "models/chat-bison-001: A conversational AI model designed to handle general-purpose dialogues, chat interactions, and more complex conversation flows. It focuses on generating coherent and contextually relevant text responses for various scenarios.\n",
        "\n",
        "*Text Generation Models*\n",
        "models/text-bison-001: This model is tailored for text generation tasks. It specializes in creating or extending written content based on a prompt, making it suitable for storytelling, blog writing, or content creation tasks.\n",
        "\n",
        "*Embedding Models*\n",
        "models/embedding-gecko-001: An embedding model used for creating dense vector representations of textual data. It’s useful for tasks like similarity matching, semantic search, and clustering.\n",
        "\n",
        "models/embedding-001: Another version of an embedding model for generating numerical embeddings from textual inputs. These embeddings are ideal for understanding the semantic meaning of texts.\n",
        "\n",
        "models/text-embedding-004: A more advanced embedding model with improved semantic understanding, suitable for natural language processing tasks, including text classification and information retrieval.\n",
        "\n",
        "*Gemini Models (Advanced Language Models)*\n",
        "These models are iterations of more advanced AI systems, likely fine-tuned for specific domains or improved with newer techniques. They represent enhancements in generative AI with increased capabilities in understanding, generating, and interacting with complex information.\n",
        "\n",
        "models/gemini-1.0-pro-latest: The latest iteration in the 1.0 series, optimized for complex tasks involving professional-quality text generation and understanding.\n",
        "\n",
        "models/gemini-1.0-pro: A high-quality generative AI model designed for professional use, emphasizing accuracy, fluency, and depth in content creation.\n",
        "\n",
        "models/gemini-pro: A generalized version focusing on professional-grade applications, suitable for business tasks or industry-specific needs.\n",
        "\n",
        "models/gemini-1.0-pro-001: A specific version within the 1.0 series, fine-tuned for professional tasks that require more precision in outputs.\n",
        "\n",
        "models/gemini-1.0-pro-vision-latest: A version that likely integrates visual understanding or data, making it capable of handling multimedia inputs like text + image.\n",
        "\n",
        "models/gemini-pro-vision: This model suggests a focus on multimodal tasks—those requiring both visual and textual understanding.\n",
        "Gemini 1.5 Models (Enhanced Capabilities)\n",
        "*These are enhanced models with more advanced capabilities, offering improvements over the 1.0 series.*\n",
        "\n",
        "models/gemini-1.5-pro-latest: The latest in the 1.5 series, optimized for professional, high-accuracy generative tasks.\n",
        "\n",
        "models/gemini-1.5-pro-001: A specific version of the 1.5 series, tuned for precise, professional applications.\n",
        "\n",
        "models/gemini-1.5-pro-002: Another variant in the 1.5 series, likely featuring more tweaks for professional contexts.\n",
        "\n",
        "models/gemini-1.5-pro: A general-purpose model in the 1.5 series, focusing on accuracy and quality for professional use cases.\n",
        "\n",
        "models/gemini-1.5-pro-exp-0801: An experimental model version released on August 1st, potentially testing new features or optimizations.\n",
        "\n",
        "models/gemini-1.5-pro-exp-0827: Another experimental model from the 1.5 series, launched on August 27th, possibly fine-tuning specific aspects of\n",
        "the model's capabilities.\n",
        "*Flash Models (Fast and Efficient)*\n",
        "\n",
        "These models focus on faster generation while maintaining good quality, making them suitable for scenarios requiring quick responses.\n",
        "\n",
        "models/gemini-1.5-flash-latest: The most recent in the 1.5 Flash series, optimized for speed while preserving accuracy.\n",
        "\n",
        "models/gemini-1.5-flash-001: An early version of the Flash series, focusing on quicker generation.\n",
        "\n",
        "models/gemini-1.5-flash-001-tuning: A version specifically adjusted for particular tasks or datasets, allowing fine control over the generation\n",
        "quality.\n",
        "\n",
        "models/gemini-1.5-flash: A general-purpose model in the Flash category, balancing speed and quality.\n",
        "\n",
        "models/gemini-1.5-flash-exp-0827: An experimental version of the Flash model released on August 27th, likely testing faster generation methods.\n",
        "\n",
        "models/gemini-1.5-flash-002: A refined version in the 1.5 Flash series, improving on its predecessors.\n",
        "*Flash 8B Models (Optimized with a Focus on Efficiency)*\n",
        "These models are part of a Flash sub-category labeled \"8B,\" possibly indicating a focus on efficiency or a particular architecture.\n",
        "\n",
        "models/gemini-1.5-flash-8b: A model in the Flash 8B series, targeting quick and efficient content generation.\n",
        "\n",
        "models/gemini-1.5-flash-8b-001: The first version in the 8B series, focusing on a balance between speed and content depth.\n",
        "\n",
        "models/gemini-1.5-flash-8b-latest: The latest version in the Flash 8B line, aiming to improve efficiency.\n",
        "\n",
        "models/gemini-1.5-flash-8b-exp-0827: An experimental 8B version released on August 27th, potentially introducing novel efficiencies.\n",
        "\n",
        "models/gemini-1.5-flash-8b-exp-0924: An experimental release in the Flash 8B series from September 24th, aiming to explore advanced generation techniques."
      ],
      "metadata": {
        "id": "Dy_2Ligb2TIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*more info of model*"
      ],
      "metadata": {
        "id": "2NA7su6E2TIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in genai.list_models():\n",
        "    if model.name=='models/gemini-1.5-flash':\n",
        "        print(model)\n",
        "        break"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T17:00:17.019323Z",
          "iopub.execute_input": "2024-11-14T17:00:17.019782Z",
          "iopub.status.idle": "2024-11-14T17:00:17.189317Z",
          "shell.execute_reply.started": "2024-11-14T17:00:17.019740Z",
          "shell.execute_reply": "2024-11-14T17:00:17.188212Z"
        },
        "id": "bpsp8j-12TIf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "***EXPLORE GENERATIVE PARAMETERS***"
      ],
      "metadata": {
        "id": "D0R_L_tY2TIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*OUTPUT LENGTH*"
      ],
      "metadata": {
        "id": "sq1mDZE52TIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "short_model=genai.GenerativeModel(\"gemini-1.5-flash\",generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
        "responce=short_model.generate_content(\"write a 1000 word essay on the importance of AI\")\n",
        "Markdown(responce.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T17:00:22.793594Z",
          "iopub.execute_input": "2024-11-14T17:00:22.794082Z",
          "iopub.status.idle": "2024-11-14T17:00:24.149259Z",
          "shell.execute_reply.started": "2024-11-14T17:00:22.794037Z",
          "shell.execute_reply": "2024-11-14T17:00:24.148080Z"
        },
        "id": "qBL8nj2T2TIf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "response=short_model.generate_content('write a short poem on life')\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T17:02:16.751151Z",
          "iopub.execute_input": "2024-11-14T17:02:16.751732Z",
          "iopub.status.idle": "2024-11-14T17:02:17.659806Z",
          "shell.execute_reply.started": "2024-11-14T17:02:16.751675Z",
          "shell.execute_reply": "2024-11-14T17:02:17.658616Z"
        },
        "id": "SqM6wEUI2TIf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEMPERATURE**"
      ],
      "metadata": {
        "id": "ikuycqAT2TIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition: Temperature modifies the probability distribution of the next word by \"sharpening\" or \"softening\" the model's predictions.\n",
        "\n",
        "How It Works: The probability of each token is adjusted by raising it to the power of 1/temperature.\n",
        "If the temperature is lower than 1 (e.g., 0.7), high-probability tokens become even more likely, making the output more deterministic.\n",
        "If the temperature is higher than 1 (e.g., 1.5), the probability distribution flattens, making less likely tokens more probable and increasing randomness.\n",
        "\n",
        "Impact:\n",
        "Controls randomness: A high temperature (greater than 1) leads to more diverse and unpredictable outputs, while a low temperature (between 0 and 1) makes the output safer and more predictable.\n",
        "\n",
        "Influences all tokens: Unlike top-k, temperature affects the entire probability distribution rather than just a subset.\n",
        "\n",
        "Example: A temperature of 0.5 makes the model more likely to choose the highest-probability token, while a temperature of 1.5 allows more variety by considering a wider range of tokens"
      ],
      "metadata": {
        "id": "9p6uxyPk2TIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "high_temp_model=genai.GenerativeModel(\"gemini-1.5-flash\",generation_config=genai.GenerationConfig(temperature=2.0))\n",
        "retry_policy={\"retry\":retry.Retry(predicate=retry.if_transient_error,initial=10,multiplier=1.5,timeout=300)}## Retry if a transient error occurs\n",
        "                                  # Start with an initial delay of 10 seconds\n",
        "                              # Multiply the delay by 1.5 after each retry\n",
        "                                   # Set a maximum total time of 300 seconds (5 minutes)\n",
        "for _ in range(5):\n",
        "    response=high_temp_model.generate_content(\"pick a random colour .....(responce in a single word)\")\n",
        "    if responce.parts:\n",
        "        print(response.text,'_'*25)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T17:14:32.940513Z",
          "iopub.execute_input": "2024-11-14T17:14:32.940974Z",
          "iopub.status.idle": "2024-11-14T17:14:34.693803Z",
          "shell.execute_reply.started": "2024-11-14T17:14:32.940930Z",
          "shell.execute_reply": "2024-11-14T17:14:34.692510Z"
        },
        "id": "5VaAQ3QL2TIg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "low_temp_model=genai.GenerativeModel(\"gemini-1.5-flash\",generation_config=genai.GenerationConfig(temperature=0.0))\n",
        "retry_policy={\"retry\":retry.Retry(predicate=retry.if_transient_error,initial=10,multiplier=1.5,timeout=300)}## Retry if a transient error occurs\n",
        "                                  # Start with an initial delay of 10 seconds\n",
        "                              # Multiply the delay by 1.5 after each retry\n",
        "                                   # Set a maximum total time of 300 seconds (5 minutes)\n",
        "for _ in range(5):\n",
        "    response=low_temp_model.generate_content(\"pick a random colour .....(responce in a single word)\")\n",
        "    if responce.parts:\n",
        "        print(response.text,'_'*25)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T17:13:53.327906Z",
          "iopub.execute_input": "2024-11-14T17:13:53.328374Z",
          "iopub.status.idle": "2024-11-14T17:13:55.132091Z",
          "shell.execute_reply.started": "2024-11-14T17:13:53.328329Z",
          "shell.execute_reply": "2024-11-14T17:13:55.130920Z"
        },
        "id": "A9jNHTw82TIg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOP-K AND TOP-P**"
      ],
      "metadata": {
        "id": "cwn-ID112TIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top-k and Top-p (also known as nucleus sampling) are two common techniques used in language models (LLMs) to control randomness and diversity in text generation. These sampling methods help generate more coherent and contextually relevant outputs by controlling how the model selects words during generation.\n",
        "\n",
        "1. Top-k Sampling\n",
        "Definition: In top-k sampling, the model considers only the top k most probable tokens (words) when generating the next word in a sentence. The rest of the words are ignored.\n",
        "\n",
        "How it works: After the model predicts a probability distribution for the next token, only the top k tokens (with the highest probabilities) are kept, and the remaining are filtered out. Then, one of the k tokens is selected randomly based on their probabilities.\n",
        "Effect: It ensures that only the most likely tokens are considered, which limits randomness but can make the text more predictable if k is small.\n",
        "Example:\n",
        "\n",
        "If k=5, only the top 5 most likely tokens are kept for random selection. This prevents the model from picking a highly unlikely token.\n",
        "2. Top-p Sampling (Nucleus Sampling)\n",
        "Definition: In top-p sampling, the model selects the smallest set of tokens whose cumulative probability is greater than or equal to a threshold p.\n",
        "\n",
        "How it works: Instead of selecting a fixed number of tokens, it considers the most probable tokens until their cumulative probability reaches p (e.g., p=0.9). The tokens are then sampled randomly based on their probabilities.\n",
        "Effect: This method allows dynamic control over how many tokens are considered, leading to more diverse outputs. When p is high (close to 1), more tokens are considered, increasing randomness.\n",
        "Example:\n",
        "\n",
        "If p=0.9, tokens are chosen until their cumulative probability is 90%. This set may contain a different number of tokens for each step, depending on the prediction.\n",
        "Key Differences\n",
        "Top-k: Limits the number of tokens considered, making it a fixed and deterministic cut-off. It provides more control over generation by setting a hard limit.\n",
        "Top-p: Uses a probability threshold, leading to a dynamic number of options. This gives more flexibility in how many words are considered, depending on the prediction's confidence"
      ],
      "metadata": {
        "id": "leK7tVyq2TIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        # These are the default values for gemini-1.5-flash-001.\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95,\n",
        "    ))\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-14T17:16:44.659531Z",
          "iopub.execute_input": "2024-11-14T17:16:44.660502Z",
          "iopub.status.idle": "2024-11-14T17:16:47.740914Z",
          "shell.execute_reply.started": "2024-11-14T17:16:44.660450Z",
          "shell.execute_reply": "2024-11-14T17:16:47.739803Z"
        },
        "id": "vQIUX7l12TIg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore available models**\n",
        "\n",
        "You will be using the embedContent API method to calculate embeddings in this guide. Find a model that supports it through the models.list endpoint. You can also find more information about the embedding models on the models page.\n",
        "\n",
        "text-embedding-004 is the most recent embedding model, so you will use it for this exercise."
      ],
      "metadata": {
        "id": "9r7KTTaA3CeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "    if \"embedContent\" in m.supported_generation_methods:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "trusted": true,
        "id": "cQfvguzw2TIh"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}