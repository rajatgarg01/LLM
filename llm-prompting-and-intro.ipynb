{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This command will install the google-generativeai package, ensuring the version is 0.8.3 or higher, and will suppress unnecessary output due to the -q flag. If you encounter any issues during installation, feel free to ask!","metadata":{}},{"cell_type":"code","source":"%pip install -U -q \"google-generativeai>=0.8.3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:43:30.238660Z","iopub.execute_input":"2024-11-14T15:43:30.239142Z","iopub.status.idle":"2024-11-14T15:43:59.805970Z","shell.execute_reply.started":"2024-11-14T15:43:30.239092Z","shell.execute_reply":"2024-11-14T15:43:59.804598Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import google.generativeai as genai\nfrom IPython.display import Markdown,display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:47:11.374500Z","iopub.execute_input":"2024-11-14T15:47:11.374910Z","iopub.status.idle":"2024-11-14T15:47:11.380391Z","shell.execute_reply.started":"2024-11-14T15:47:11.374870Z","shell.execute_reply":"2024-11-14T15:47:11.379080Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"HTML: The heading \"Welcome to IPython Display Examples!\" will be displayed in green color using HTML formatting.\nMarkdown: A bulleted list, link, code snippet, and blockquote are demonstrated using Markdown syntax.\ndisplay: This function is used to render both html_content and markdown_content in a single output cell.\nThis code should work seamlessly in a Jupyter Notebook, giving you a combined output of HTML and Markdown elements!","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nGOOGLE_API_KEY=UserSecretsClient().get_secret(\"google_api_key\")\ngenai.configure(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:52:09.077462Z","iopub.execute_input":"2024-11-14T15:52:09.077953Z","iopub.status.idle":"2024-11-14T15:52:09.244555Z","shell.execute_reply.started":"2024-11-14T15:52:09.077896Z","shell.execute_reply":"2024-11-14T15:52:09.243359Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"The code snippet you've posted suggests that you are trying to access a Google API key securely using the UserSecretsClient and configure the genai library with that key. Here's a breakdown of how the code works:\n\nCode Explanation\nUserSecretsClient: This is often used in environments like Jupyter Notebooks (e.g., Azure Notebooks or JupyterHub) where secrets (like API keys) are stored securely. You retrieve a secret using the get_secret method, which fetches a secret (in this case, \"GOOGLE_API_KEY\").\n\nAPI Configuration: The genai.configure() function is used to set up the google-generativeai library with the retrieved API key for making authenticated API requests","metadata":{}},{"cell_type":"markdown","source":"**FIRST PROMPT**","metadata":{}},{"cell_type":"code","source":"flash=genai.GenerativeModel(\"gemini-1.5-flash\")\nresponse=flash.generate_content(\"Explain AI to me like I'm a kid\")\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:55:37.649734Z","iopub.execute_input":"2024-11-14T15:55:37.650128Z","iopub.status.idle":"2024-11-14T15:55:39.148431Z","shell.execute_reply.started":"2024-11-14T15:55:37.650090Z","shell.execute_reply":"2024-11-14T15:55:39.147370Z"}},"outputs":[{"name":"stdout","text":"Imagine you have a super smart robot that can learn things like you do! That's kind of what AI, or Artificial Intelligence, is. It's like a computer brain that can learn from lots of information and use that knowledge to do amazing things.\n\nThink of a video game you love. The characters in that game might be controlled by AI. They learn your moves and try to beat you! Or, maybe you use a voice assistant like Alexa or Siri. They use AI to understand your words and answer your questions.\n\nAI is still learning and growing, just like you! But it's already helping us in many ways, from making our phones smarter to helping doctors find new cures for diseases. It's like having a superpowered friend who can help us do amazing things! \n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T15:56:20.669672Z","iopub.execute_input":"2024-11-14T15:56:20.670130Z","iopub.status.idle":"2024-11-14T15:56:20.679703Z","shell.execute_reply.started":"2024-11-14T15:56:20.670088Z","shell.execute_reply":"2024-11-14T15:56:20.678354Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"Imagine you have a super smart robot that can learn things like you do! That's kind of what AI, or Artificial Intelligence, is. It's like a computer brain that can learn from lots of information and use that knowledge to do amazing things.\n\nThink of a video game you love. The characters in that game might be controlled by AI. They learn your moves and try to beat you! Or, maybe you use a voice assistant like Alexa or Siri. They use AI to understand your words and answer your questions.\n\nAI is still learning and growing, just like you! But it's already helping us in many ways, from making our phones smarter to helping doctors find new cures for diseases. It's like having a superpowered friend who can help us do amazing things! \n"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"***START A CHAT***","metadata":{}},{"cell_type":"code","source":"chat=flash.start_chat(history=[])\nresponse=chat.send_message('Hello! my name is rajat garg')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:00:18.988919Z","iopub.execute_input":"2024-11-14T16:00:18.989452Z","iopub.status.idle":"2024-11-14T16:00:19.452810Z","shell.execute_reply.started":"2024-11-14T16:00:18.989394Z","shell.execute_reply":"2024-11-14T16:00:19.451659Z"}},"outputs":[{"name":"stdout","text":"Hello Rajat! Nice to meet you. ðŸ˜Š What can I do for you today? \n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"response=chat.send_message('can you tell me something intresting about you')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:55.412368Z","iopub.execute_input":"2024-11-14T16:01:55.412995Z","iopub.status.idle":"2024-11-14T16:01:56.529642Z","shell.execute_reply.started":"2024-11-14T16:01:55.412938Z","shell.execute_reply":"2024-11-14T16:01:56.528296Z"}},"outputs":[{"name":"stdout","text":"That's a great question! It's hard to say what's \"interesting\" about me, as I'm not a person. I'm a large language model, which means I'm a computer program trained on a massive amount of text data. That lets me do some pretty cool things, like:\n\n* **Generate different creative text formats**: poems, code, scripts, musical pieces, email, letters, etc. \n* **Answer your questions in an informative way**: even if they're open ended, challenging, or strange.\n* **Translate languages**: I can translate between many different languages.\n\nThe most interesting thing about me, I think, is that I can learn and grow over time. The more data I'm trained on, the better I become at understanding and responding to your requests. \n\nSo, what would you like to know about me? ðŸ˜Š \n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"Markdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:02:15.513836Z","iopub.execute_input":"2024-11-14T16:02:15.514422Z","iopub.status.idle":"2024-11-14T16:02:15.523467Z","shell.execute_reply.started":"2024-11-14T16:02:15.514364Z","shell.execute_reply":"2024-11-14T16:02:15.522152Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"That's a great question! It's hard to say what's \"interesting\" about me, as I'm not a person. I'm a large language model, which means I'm a computer program trained on a massive amount of text data. That lets me do some pretty cool things, like:\n\n* **Generate different creative text formats**: poems, code, scripts, musical pieces, email, letters, etc. \n* **Answer your questions in an informative way**: even if they're open ended, challenging, or strange.\n* **Translate languages**: I can translate between many different languages.\n\nThe most interesting thing about me, I think, is that I can learn and grow over time. The more data I'm trained on, the better I become at understanding and responding to your requests. \n\nSo, what would you like to know about me? ðŸ˜Š \n"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"response=chat.send_message('do you still remember my name')\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:03:28.689836Z","iopub.execute_input":"2024-11-14T16:03:28.690399Z","iopub.status.idle":"2024-11-14T16:03:29.317308Z","shell.execute_reply.started":"2024-11-14T16:03:28.690339Z","shell.execute_reply":"2024-11-14T16:03:29.315899Z"}},"outputs":[{"name":"stdout","text":"Of course!  I remember your name is Rajat Garg. ðŸ˜Š  \n\nI'm designed to keep track of conversations and remember information like names, so I can have more natural interactions.  Is there anything else you'd like to talk about today? \n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**CHOOSE A MODEL**","metadata":{}},{"cell_type":"code","source":"for model in genai.list_models():\n    print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:05:28.920741Z","iopub.execute_input":"2024-11-14T16:05:28.921204Z","iopub.status.idle":"2024-11-14T16:05:29.084888Z","shell.execute_reply.started":"2024-11-14T16:05:28.921158Z","shell.execute_reply":"2024-11-14T16:05:29.083491Z"}},"outputs":[{"name":"stdout","text":"models/chat-bison-001\nmodels/text-bison-001\nmodels/embedding-gecko-001\nmodels/gemini-1.0-pro-latest\nmodels/gemini-1.0-pro\nmodels/gemini-pro\nmodels/gemini-1.0-pro-001\nmodels/gemini-1.0-pro-vision-latest\nmodels/gemini-pro-vision\nmodels/gemini-1.5-pro-latest\nmodels/gemini-1.5-pro-001\nmodels/gemini-1.5-pro-002\nmodels/gemini-1.5-pro\nmodels/gemini-1.5-pro-exp-0801\nmodels/gemini-1.5-pro-exp-0827\nmodels/gemini-1.5-flash-latest\nmodels/gemini-1.5-flash-001\nmodels/gemini-1.5-flash-001-tuning\nmodels/gemini-1.5-flash\nmodels/gemini-1.5-flash-exp-0827\nmodels/gemini-1.5-flash-002\nmodels/gemini-1.5-flash-8b\nmodels/gemini-1.5-flash-8b-001\nmodels/gemini-1.5-flash-8b-latest\nmodels/gemini-1.5-flash-8b-exp-0827\nmodels/gemini-1.5-flash-8b-exp-0924\nmodels/embedding-001\nmodels/text-embedding-004\nmodels/aqa\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Here's a description of each AI model you mentioned, categorized based on their functions:\n\n*Chat Models*\nmodels/chat-bison-001: A conversational AI model designed to handle general-purpose dialogues, chat interactions, and more complex conversation flows. It focuses on generating coherent and contextually relevant text responses for various scenarios.\n\n*Text Generation Models*\nmodels/text-bison-001: This model is tailored for text generation tasks. It specializes in creating or extending written content based on a prompt, making it suitable for storytelling, blog writing, or content creation tasks.\n\n*Embedding Models*\nmodels/embedding-gecko-001: An embedding model used for creating dense vector representations of textual data. Itâ€™s useful for tasks like similarity matching, semantic search, and clustering.\n\nmodels/embedding-001: Another version of an embedding model for generating numerical embeddings from textual inputs. These embeddings are ideal for understanding the semantic meaning of texts.\n\nmodels/text-embedding-004: A more advanced embedding model with improved semantic understanding, suitable for natural language processing tasks, including text classification and information retrieval.\n\n*Gemini Models (Advanced Language Models)*\nThese models are iterations of more advanced AI systems, likely fine-tuned for specific domains or improved with newer techniques. They represent enhancements in generative AI with increased capabilities in understanding, generating, and interacting with complex information.\n\nmodels/gemini-1.0-pro-latest: The latest iteration in the 1.0 series, optimized for complex tasks involving professional-quality text generation and understanding.\n\nmodels/gemini-1.0-pro: A high-quality generative AI model designed for professional use, emphasizing accuracy, fluency, and depth in content creation.\n\nmodels/gemini-pro: A generalized version focusing on professional-grade applications, suitable for business tasks or industry-specific needs.\n\nmodels/gemini-1.0-pro-001: A specific version within the 1.0 series, fine-tuned for professional tasks that require more precision in outputs.\n\nmodels/gemini-1.0-pro-vision-latest: A version that likely integrates visual understanding or data, making it capable of handling multimedia inputs like text + image.\n\nmodels/gemini-pro-vision: This model suggests a focus on multimodal tasksâ€”those requiring both visual and textual understanding.\nGemini 1.5 Models (Enhanced Capabilities)\n*These are enhanced models with more advanced capabilities, offering improvements over the 1.0 series.*\n\nmodels/gemini-1.5-pro-latest: The latest in the 1.5 series, optimized for professional, high-accuracy generative tasks.\n\nmodels/gemini-1.5-pro-001: A specific version of the 1.5 series, tuned for precise, professional applications.\n\nmodels/gemini-1.5-pro-002: Another variant in the 1.5 series, likely featuring more tweaks for professional contexts.\n\nmodels/gemini-1.5-pro: A general-purpose model in the 1.5 series, focusing on accuracy and quality for professional use cases.\n\nmodels/gemini-1.5-pro-exp-0801: An experimental model version released on August 1st, potentially testing new features or optimizations.\n\nmodels/gemini-1.5-pro-exp-0827: Another experimental model from the 1.5 series, launched on August 27th, possibly fine-tuning specific aspects of \nthe model's capabilities.\n*Flash Models (Fast and Efficient)*\n\nThese models focus on faster generation while maintaining good quality, making them suitable for scenarios requiring quick responses.\n\nmodels/gemini-1.5-flash-latest: The most recent in the 1.5 Flash series, optimized for speed while preserving accuracy.\n\nmodels/gemini-1.5-flash-001: An early version of the Flash series, focusing on quicker generation.\n\nmodels/gemini-1.5-flash-001-tuning: A version specifically adjusted for particular tasks or datasets, allowing fine control over the generation \nquality.\n\nmodels/gemini-1.5-flash: A general-purpose model in the Flash category, balancing speed and quality.\n\nmodels/gemini-1.5-flash-exp-0827: An experimental version of the Flash model released on August 27th, likely testing faster generation methods.\n\nmodels/gemini-1.5-flash-002: A refined version in the 1.5 Flash series, improving on its predecessors.\n*Flash 8B Models (Optimized with a Focus on Efficiency)*\nThese models are part of a Flash sub-category labeled \"8B,\" possibly indicating a focus on efficiency or a particular architecture.\n\nmodels/gemini-1.5-flash-8b: A model in the Flash 8B series, targeting quick and efficient content generation.\n\nmodels/gemini-1.5-flash-8b-001: The first version in the 8B series, focusing on a balance between speed and content depth.\n\nmodels/gemini-1.5-flash-8b-latest: The latest version in the Flash 8B line, aiming to improve efficiency.\n\nmodels/gemini-1.5-flash-8b-exp-0827: An experimental 8B version released on August 27th, potentially introducing novel efficiencies.\n\nmodels/gemini-1.5-flash-8b-exp-0924: An experimental release in the Flash 8B series from September 24th, aiming to explore advanced generation techniques.","metadata":{}},{"cell_type":"markdown","source":"*more info of model*","metadata":{}},{"cell_type":"code","source":"for model in genai.list_models():\n    if model.name=='models/gemini-1.5-flash':\n        print(model)\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:00:17.019323Z","iopub.execute_input":"2024-11-14T17:00:17.019782Z","iopub.status.idle":"2024-11-14T17:00:17.189317Z","shell.execute_reply.started":"2024-11-14T17:00:17.019740Z","shell.execute_reply":"2024-11-14T17:00:17.188212Z"}},"outputs":[{"name":"stdout","text":"Model(name='models/gemini-1.5-flash',\n      base_model_id='',\n      version='001',\n      display_name='Gemini 1.5 Flash',\n      description='Fast and versatile multimodal model for scaling across diverse tasks',\n      input_token_limit=1000000,\n      output_token_limit=8192,\n      supported_generation_methods=['generateContent', 'countTokens'],\n      temperature=1.0,\n      max_temperature=2.0,\n      top_p=0.95,\n      top_k=40)\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"***EXPLORE GENERATIVE PARAMETERS***","metadata":{}},{"cell_type":"markdown","source":"*OUTPUT LENGTH*","metadata":{}},{"cell_type":"code","source":"short_model=genai.GenerativeModel(\"gemini-1.5-flash\",generation_config=genai.GenerationConfig(max_output_tokens=200))\nresponce=short_model.generate_content(\"write a 1000 word essay on the importance of AI\")\nMarkdown(responce.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:00:22.793594Z","iopub.execute_input":"2024-11-14T17:00:22.794082Z","iopub.status.idle":"2024-11-14T17:00:24.149259Z","shell.execute_reply.started":"2024-11-14T17:00:22.794037Z","shell.execute_reply":"2024-11-14T17:00:24.148080Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## The Rise of the Machines: Unpacking the Importance of Artificial Intelligence\n\nArtificial intelligence (AI) has gone from a futuristic fantasy to a ubiquitous reality in a remarkably short time. From personalized recommendations on streaming platforms to self-driving cars on our roads, AI is quietly but profoundly transforming every facet of our lives. The significance of AI transcends mere convenience; it holds the potential to revolutionize industries, address global challenges, and ultimately redefine the very nature of human existence.\n\nThe importance of AI can be understood through its impact on various domains:\n\n**1. Economic Growth and Efficiency:** AI promises to unlock unprecedented levels of economic growth by automating tasks, optimizing processes, and fostering innovation. From automating repetitive tasks in manufacturing and logistics to providing personalized experiences in customer service and retail, AI frees up human resources for higher-level tasks, leading to increased productivity and efficiency. The potential of AI in automating tasks and improving operational efficiency extends to healthcare, finance, and even scientific research, paving the way for"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"response=short_model.generate_content('write a short poem on life')\nMarkdown(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:02:16.751151Z","iopub.execute_input":"2024-11-14T17:02:16.751732Z","iopub.status.idle":"2024-11-14T17:02:17.659806Z","shell.execute_reply.started":"2024-11-14T17:02:16.751675Z","shell.execute_reply":"2024-11-14T17:02:17.658616Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"A fragile breath, a fleeting spark,\nA tapestry of joy and dark.\nFrom tender bloom to fading leaf,\nA journey brief, a bittersweet grief.\n\nWe build and break, we love and lose,\nThrough sunlit days and moonlit muse.\nEach moment etched, a precious grain,\nIn life's grand dance, a fleeting stain.\n\nSo cherish now, this fleeting grace,\nEmbrace the light, find solace in its space.\nFor life's a song, a whispered rhyme,\nA fleeting gift, a precious time. \n"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"**TEMPERATURE**","metadata":{}},{"cell_type":"markdown","source":"Definition: Temperature modifies the probability distribution of the next word by \"sharpening\" or \"softening\" the model's predictions.\n\nHow It Works: The probability of each token is adjusted by raising it to the power of 1/temperature.\nIf the temperature is lower than 1 (e.g., 0.7), high-probability tokens become even more likely, making the output more deterministic.\nIf the temperature is higher than 1 (e.g., 1.5), the probability distribution flattens, making less likely tokens more probable and increasing randomness.\n\nImpact:\nControls randomness: A high temperature (greater than 1) leads to more diverse and unpredictable outputs, while a low temperature (between 0 and 1) makes the output safer and more predictable.\n\nInfluences all tokens: Unlike top-k, temperature affects the entire probability distribution rather than just a subset.\n\nExample: A temperature of 0.5 makes the model more likely to choose the highest-probability token, while a temperature of 1.5 allows more variety by considering a wider range of tokens","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\nhigh_temp_model=genai.GenerativeModel(\"gemini-1.5-flash\",generation_config=genai.GenerationConfig(temperature=2.0))\nretry_policy={\"retry\":retry.Retry(predicate=retry.if_transient_error,initial=10,multiplier=1.5,timeout=300)}## Retry if a transient error occurs\n                                  # Start with an initial delay of 10 seconds\n                              # Multiply the delay by 1.5 after each retry\n                                   # Set a maximum total time of 300 seconds (5 minutes)\nfor _ in range(5):\n    response=high_temp_model.generate_content(\"pick a random colour .....(responce in a single word)\")\n    if responce.parts:\n        print(response.text,'_'*25)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:14:32.940513Z","iopub.execute_input":"2024-11-14T17:14:32.940974Z","iopub.status.idle":"2024-11-14T17:14:34.693803Z","shell.execute_reply.started":"2024-11-14T17:14:32.940930Z","shell.execute_reply":"2024-11-14T17:14:34.692510Z"}},"outputs":[{"name":"stdout","text":"Purple. \n _________________________\nPurple \n _________________________\nBlue \n _________________________\nBlue \n _________________________\nIndigo \n _________________________\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"from google.api_core import retry\nlow_temp_model=genai.GenerativeModel(\"gemini-1.5-flash\",generation_config=genai.GenerationConfig(temperature=0.0))\nretry_policy={\"retry\":retry.Retry(predicate=retry.if_transient_error,initial=10,multiplier=1.5,timeout=300)}## Retry if a transient error occurs\n                                  # Start with an initial delay of 10 seconds\n                              # Multiply the delay by 1.5 after each retry\n                                   # Set a maximum total time of 300 seconds (5 minutes)\nfor _ in range(5):\n    response=low_temp_model.generate_content(\"pick a random colour .....(responce in a single word)\")\n    if responce.parts:\n        print(response.text,'_'*25)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:13:53.327906Z","iopub.execute_input":"2024-11-14T17:13:53.328374Z","iopub.status.idle":"2024-11-14T17:13:55.132091Z","shell.execute_reply.started":"2024-11-14T17:13:53.328329Z","shell.execute_reply":"2024-11-14T17:13:55.130920Z"}},"outputs":[{"name":"stdout","text":"Blue \n _________________________\nBlue \n _________________________\nBlue \n _________________________\nBlue \n _________________________\nBlue \n _________________________\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"**TOP-K AND TOP-P**","metadata":{}},{"cell_type":"markdown","source":"Top-k and Top-p (also known as nucleus sampling) are two common techniques used in language models (LLMs) to control randomness and diversity in text generation. These sampling methods help generate more coherent and contextually relevant outputs by controlling how the model selects words during generation.\n\n1. Top-k Sampling\nDefinition: In top-k sampling, the model considers only the top k most probable tokens (words) when generating the next word in a sentence. The rest of the words are ignored.\n\nHow it works: After the model predicts a probability distribution for the next token, only the top k tokens (with the highest probabilities) are kept, and the remaining are filtered out. Then, one of the k tokens is selected randomly based on their probabilities.\nEffect: It ensures that only the most likely tokens are considered, which limits randomness but can make the text more predictable if k is small.\nExample:\n\nIf k=5, only the top 5 most likely tokens are kept for random selection. This prevents the model from picking a highly unlikely token.\n2. Top-p Sampling (Nucleus Sampling)\nDefinition: In top-p sampling, the model selects the smallest set of tokens whose cumulative probability is greater than or equal to a threshold p.\n\nHow it works: Instead of selecting a fixed number of tokens, it considers the most probable tokens until their cumulative probability reaches p (e.g., p=0.9). The tokens are then sampled randomly based on their probabilities.\nEffect: This method allows dynamic control over how many tokens are considered, leading to more diverse outputs. When p is high (close to 1), more tokens are considered, increasing randomness.\nExample:\n\nIf p=0.9, tokens are chosen until their cumulative probability is 90%. This set may contain a different number of tokens for each step, depending on the prediction.\nKey Differences\nTop-k: Limits the number of tokens considered, making it a fixed and deterministic cut-off. It provides more control over generation by setting a hard limit.\nTop-p: Uses a probability threshold, leading to a dynamic number of options. This gives more flexibility in how many words are considered, depending on the prediction's confidence","metadata":{}},{"cell_type":"code","source":"model = genai.GenerativeModel(\n    'gemini-1.5-flash-001',\n    generation_config=genai.GenerationConfig(\n        # These are the default values for gemini-1.5-flash-001.\n        temperature=1.0,\n        top_k=64,\n        top_p=0.95,\n    ))\n\nstory_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\nresponse = model.generate_content(story_prompt, request_options=retry_policy)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T17:16:44.659531Z","iopub.execute_input":"2024-11-14T17:16:44.660502Z","iopub.status.idle":"2024-11-14T17:16:47.740914Z","shell.execute_reply.started":"2024-11-14T17:16:44.660450Z","shell.execute_reply":"2024-11-14T17:16:47.739803Z"}},"outputs":[{"name":"stdout","text":"Bartholomew, a ginger tabby with a disdain for routine, found himself staring out the window at the rain-slicked street. He sighed, a heavy, exasperated sigh that made the curtains rustle. His human, a kindly old lady named Mrs. Peabody, was knitting by the fire, blissfully unaware of the grand adventure brewing in Bartholomew's mind.\n\nHe had never been beyond Mrs. Peabody's garden, a realm of neatly trimmed hedges and brightly coloured geraniums. But today, the rain, the wind, the very air seemed to whisper of unknown lands.  \n\nWith a decisive flick of his tail, Bartholomew leapt onto the window sill, his claws finding purchase on the worn wooden frame. He nudged the latch open, a forbidden act that sent a jolt of fear through him, followed by a surge of thrill. \n\nThe world outside was a symphony of dripping leaves, swirling puddles, and the melancholic cries of pigeons. He crept out, his ginger fur blending with the wet shadows. The air was filled with the scent of damp earth and the faintest hint of jasmine. \n\nThe adventure began slowly, a meandering walk along cobblestone alleys, punctuated by the startled squawks of birds. Bartholomew, with his sharp senses, discovered a world teeming with hidden wonders: a secret garden with a fountain cascading crystal water, a friendly stray dog who offered him a juicy bone, and a whole family of mice who scurried out from their hiding place, intrigued by his presence. \n\nHe found himself in a bustling marketplace, a cacophony of voices, the smell of spices, and the sight of brightly coloured fabrics. He dodged a clumsy baker's cart, felt the warmth of a baker's hand as he received a stray pastry, and even had a brief but intense staring contest with a grumpy but majestic bulldog. \n\nAs the sun began to set, painting the sky with hues of orange and pink, Bartholomew realised it was time to return. He had seen the world, felt the rain, tasted the sweetness of adventure. It was time to go home, to Mrs. Peabody's warm fire, and to the comfort of his familiar bed.\n\nHe crept back through the window, leaving behind a world of wonder, but carrying within him the spirit of adventure. Bartholomew, the ginger cat, had finally found his own little corner of the world, a place where routine and excitement could co-exist, a place where even a simple cat could have grand adventures. And as he curled up on Mrs. Peabody's lap, the warmth of the fire reflecting in his eyes, he knew, with a quiet purr, that his life had been forever changed. \n\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}